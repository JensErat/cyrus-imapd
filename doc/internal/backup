Notes for backup implementation

Backup index database (one per user):

backup:
    int id
    timestamp ts
    int offset
    int length

backup_mailbox:
    int id
    int backup_id
    char uniqueid
    char mboxname
    int last_uid
    int highestmodseq
    int recentuid
    timestamp recenttime
    timestamp last_appenddate
    ? pop3_last_login
    ? pop3_show_after
    timestamp uidvalidity
    char partition
    char acl
    char options
    int sync_crc
    int sync_crc_annot
    char quotaroot
    int xconvmodseq
    [annotations]

mailbox_record:
    char mailbox_uniqueid
    int uid
    int modseq
    timestamp last_updated
    [flags]
    timestamp internaldate
    int size
    char guid
    [annotations]
    boolean expunged # duplicated from [flags]?

message:
    int id
    char partition
    char guid
    int backup_id
    int offset
    int length

sieve scripts and messages are both identified by a GUID
but APPLY SIEVE doesn't take a GUID, it seems to be generated locally?
the GUID in the response to APPLY SIEVE is generated in the process of
reading the script from disk (sync_sieve_list_generate)


do i need a small table with the current state of the mailboxes, just pointing to the
most recent backup_mailbox record for each? like:

mailbox:
    char uniqueid
    char mboxname
    int backup_mailbox_id (fk backup_mailbox.id)


messages
--------

APPLY MESSAGE is a list of messages, not necessarily only one message.

>>> hmm. it might be a list of messages for -multiple users-...

Cheap first implementation is to index the start/end of the entire APPLY
MESSAGE command identically for each message within it, and at restore time
we grab that chunk and loop over it looking for the correct guid.

Ideal implementation would be to index the offset and length of each message
exactly (even excluding the dlist wrapper), but this is rather complicated
by the dlist API.

DL_SFILE type within dlist will have the correct offset/size when parsed from
an existing dlist out of a backup file (i.e. for reindex).  But for actually
creating the backup, the offset it will obtain is the offset in the connection
from the sync_client, which is not useful to us -- we need the offset within
the gzip stream that we are producing.

Compressing will need to consider this too...

backup_append now tracks where it's up to, so i don't think DLIST_SFILE even
needs to care about the offset anymore.  reindex never used it...

we just index the offset and length of the dlist entry for the message, and
we can parse the pure message data back out later from that, when we need to.
slightly less efficient on reads, but works->good->fast.  but this is
forgetting that a MESSAGE dlist has many messages, not just one...

what if a DL_SFILE's offset was the byte offset within the entire dlist
structure?  not the offset within the stream that produced it, but just within
the dlist it's part of?  then the message offset in the data stream would be the
dlist's offset in the stream plus the DL_SFILE's offset within the dlist

... nah, short of changing the data format i think it's best to index the
APPLY MESSAGE command, and search it by GUID for the message in question if/when
the message content is needed.  this cooperates nicely with reserve/message's
multiplicity and makes compress easier to implement.


renames
-------

APPLY RENAME %(OLDMBOXNAME old NEWMBOXNAME new PARTITION p UIDVALIDITY 123)

We identify mboxes by uniqueid, so when we start seeing sync data for the same
uniqueid with a new mboxname we just transparently update it anyway, without
needing to handle the APPLY RENAME.  Not sure if this is a problem...  Do we
need to record an mbox's previous names somehow?

I think it's possible to use this to rename a USER though, something like:

APPLY RENAME %(OLDMBOXNAME example.com!user.smithj NEWMBOXNAME example.com!user.jsmith ...)

-- in which case, without special handling of the RENAME command itself, there
will be a backup for the old user that ends with the RENAME, and a backup of
the new user that (probably) duplicates everything again (except for stuff
that's been expunged).

And if someone else gets given the original name, like

APPLY RENAME %(OLDMBOXNAME example.com!user.samantha-mithj NEWMBOXNAME example.com!user.smithj ...)

Then anything that was expunged from the original user but still available in
backup disappears?  Or the two backups get conflated, and samantha can
"restore" the original smithj's old mail?

Uggh.

if there's a mailboxes database pointing to the backup files, then the backup
file names don't need to be based on the userid, they could e.g. be based on
the user's inbox's uniqueid.  this would make it easier to deal with user
renames because the backup filename wouldn't need to change.  but this depends
on the uniqueid(s) in question being present on most areas of the sync
protocol, otherwise when starting a backup of a brand new user we won't be
able to tell where to store it.  workaround in the meantime could be to make
some kind of backup id from the mailboxes database, and base the filename on
this.

actually, using "some kind of backup id from the mailboxes database" is probably
the best solution.  otherwise the lock complexity of renaming a user while making
sure their new backup filename doesn't already exist is frightful.

maybe do something with mkstemp()?

furthermore: what if a mailbox is moved from one user to another?  like:

APPLY RENAME %(OLD... example.com!user.foo.something NEW... example.com!user.bar.something ...)

when a different-uid rename IS a rename of a user (and not just a folder
being moved to a different user), what does it look like?
* does it do a single APPLY RENAME for the user, and expect their folders to
  shake out of that?
* does it do an APPLY RENAME for each of their folders?

in the latter case, we need to append each of those RENAMEs to the old backup
so they can take effect correctly, and THEN rename the backup file itself. but
how to tell when the appends are finished?

how can we tell the difference between folder(s) moved to a different user vs
user has been renamed?

there is a setting: 'allowusermoves: 0' which, when enabled, allows users to
be renamed via IMAP rename/xfer commands.  but the default is that this is
disabled.  we could initially require this to be disabled while using backups...

not sure what the workflow looks like for renaming a user if this is not enabled.

not sure what the sync flow looks like in either case.

looking at sync_apply_rename and mboxlist_renamemailbox, it seems like we'll
see an APPLY RENAME for each affected mbox when a recursive rename is occurring.

there doesn't seem to be anything preventing user/a/foo -> user/b/foo in the
general (non-INBOX) case.


locking
-------

just use a normal flock/fcntl lock on the data file and only open the index
if that lock succeeded

backup:   needs to append foo and update foo.index
reindex:  only needs to read foo, but needs a write lock to prevent writes
          while it does so. needs to write to (replace) foo.index
compress: needs to re-write foo and foo.index
restore:  needs to read


verifying index
---------------

how to tell whether the .index file is the correct one for the backup data it
ostensibly represents?

one way to do this would be to have backup_index_end() store a checksum of
the corresponding data contents in the index.

when opening a backup, verify this checksum against the data, and refuse to
load the index if it doesn't match.

- sha1sum of (compressed) contents of file prior to each chunk

how to tell whether the chunk data is any good?  store a checksum of the chunk
contents along with the rest of the chunk index

- sha1sum of (uncompressed) contents of each chunk


mailboxes database
------------------

bron reckons use twoskip for this
userid -> backup_filename

lib/cyrusdb module implements this, look into that

look at conversations db code to see how to use it

need a tool:
    * given a user, show their backup filename
    * dump/undump
    * rebuild based on files discovered in backup directory

where does this fit into the locking scheme?


reindex
-------

* convert user mailbox name to backup name
* complain if there's no backup data file?
* lock, rename .index to .index.old, init new .index
* foreach file chunk:
*   timestamp is from first line in chunk
*   complain if timestamp has gone backwards?
*   index records from chunk
* unlock
* clean up .index.old

on error:
* discard partial new index
* restore .index.old
* bail out


backupd
-------

cmdloop:
* (periodic cleanup)
* read command, determine backup name
* already holding lock ? bump timestamp : obtain lock
* write data to gzname, flush immediately
* index data

periodic cleanup:
* check timestamp of each held lock
* if stale (define: stale?), release

sync restart:
* release each held lock

exit:
* release each held lock

need a "backup_index_abort" to complete the backup_index_start/end set.
_start should create a transaction, _end should commit it, and _abort should
roll it back.  then, if backupd fails to write to the gzip file for some
reason, the (now invalid) index info we added can be discarded too.

flushing immediately on write results in poor gzip compression, but for
incremental backups that's not a problem.  when the compress process hits the
file it will recompress the data more efficiently.


sql snippets
-----------

sqlite> select id, backup_id, substr(uniqueid,1,6),mboxname from backup_mailbox;
id|backup_id|substr(uniqueid,1,6)|mboxname
1|1|c9b238|fastmail.com!user.seleniumbetauser5
2|2|c9b238|fastmail.com!user.seleniumbetauser5
3|3|c9b238|fastmail.com!user.seleniumbetauser5
4|3|1475ff|fastmail.com!user.seleniumbetauser5.Archive
5|4|afd5c4|fastmail.com!user.seleniumbetauser5.#calendars.b7eb1eab-a6dc-4ef5-b44f-ac67efcfb5e0
6|4|6e15a6|fastmail.com!user.seleniumbetauser5.testfolder
7|5|c9b238|fastmail.com!user.seleniumbetauser5
8|5|1475ff|fastmail.com!user.seleniumbetauser5.Archive
9|5|6e15a6|fastmail.com!user.seleniumbetauser5.testfolder renamed
sqlite> select bm.id, z.backup_id, substr(bm.uniqueid,1,6), bm.mboxname from backup_mailbox as bm inner join ( select uniqueid, max(backup_id) as backup_id from backup_mailbox group by uniqueid) as z on bm.uniqueid = z.uniqueid and bm.backup_id = z.backup_id;
id|backup_id|substr(bm.uniqueid,1,6)|mboxname
8|5|1475ff|fastmail.com!user.seleniumbetauser5.Archive
9|5|6e15a6|fastmail.com!user.seleniumbetauser5.testfolder renamed
5|4|afd5c4|fastmail.com!user.seleniumbetauser5.#calendars.b7eb1eab-a6dc-4ef5-b44f-ac67efcfb5e0
7|5|c9b238|fastmail.com!user.seleniumbetauser5


redesigned index database
-------------------------

i don't think i need to index every version of every mailbox, just the most recent version

backup:
    int id
    timestamp ts
    int offset
    int length
    text file_sha1              -> sha1 of (compressed) data prior to this backup
    text data_sha1              -> sha1 of (uncompressed) data contained in this backup

mailbox:
    int id
    int last_backup_id
    char uniqueid               -> unique
    char mboxname               -> altered by a rename
    char mboxtype
    int last_uid
    int highestmodseq
    int recentuid
    timestamp recenttime
    timestamp last_appenddate
    timestamp pop3_last_login
    timestamp pop3_show_after
    timestamp uidvalidity
    char partition
    char acl
    char options
    int sync_crc
    int sync_crc_annot
    char quotaroot
    int xconvmodseq
    char annotations
    timestamp deleted           -> set by unmailbox i guess

message:
    int id
    char guid
    char partition              -> this is used to set the spool directory for the temp file - we might not need it
    int backup_id
    int offset
    int length

mailbox_message:
    int mailbox_id
    int message_id
    int last_backup_id          -> mailbox has its own one of these, not sure if this needs it too?
    int uid
    int modseq
    timestamp last_updated
    char flags
    timestamp internaldate
    int size
    char annotations
    timestamp expunged          -> time that it was expunged, or 0 if still alive


questions
---------
* what does it look like when uidvalidity changes?


restore
-------

* restored: runs on master servers listening for sync commands from the restore client
* restore: sbin program for selecting and restoring messages

restoration is effectively a reverse-direction replication (replicating TO master),
which means we can't supply things like uid, modseq, etc without racing against normal
message arrivals.  so instead we add an extra command to the protocol to restore a
message to a folder but let the server determine the tasty bits.

protocol flow looks something like:

c: APPLY RESERVE ... # as usual
s: * MISSING (foo bar)
s: OK
c: APPLY MESSAGE ... # as usual
s: OK
c: [new command: APPLY RESTORE? RESTORE MAILBOX?]
s: OK

we introduce a new command, RESTORE MAILBOX, which is similar to an inversion of the
usual APPLY MAILBOX.  i.e. instead of specifying the mailbox state plus a list of
message records indexed by uid, we instead supply the message guid plus a list of
mailbox-records describing the mailboxes it should be added to and the flags/annotations
for each one.  the server adds it into the mailbox as per delivery of a new message
-- generating uid, modseq, etc itself, rather than receiving them from the restore client

this will end up generating new events in the backup channel's sync log, and then the
messages will be backed up again with their new uids, etc.  additional wire transfer
of message data should be avoided by keeping the same guid.


compress
--------

remember: a single APPLY MESSAGE line can contain many messages!

settings:

* backup retention period
* chunk combination size (byte length or elapsed time)

four kinds of compression (probably at least two simultaneously):

* removing unused chunks
* combining adjacent chunks into a single chunk (for better gz compression)
* removing unused message lines from within a chunk (important after combining)
* removing unused messages from within a message line

"unused messages"
    messages for which all records have been expunged for longer
    than the retention period
"unused chunks"
    chunks which contain only unused messages

algorithm:

*   open (and lock) backup and backup.new (or bail out)
*   use backup index to identify chunks we still need
*   create a chunk in backup.new
*   foreach chunk we still need:
*       foreach line in the chunk:
*           create new line
*           foreach message in line:
*               if we still need the message, or if we're not doing message granularity
*                   add the message to the new line
*           write and index tmp line to backup.new
*       if the new chunk is big enough, or if we're not combining
*           end chunk and start a new one
*   end the new chunk
*   rename backup->backup.old, backup.new->backup
*   close (and unlock) backup.old and backup


bu_lock
-------

command line utility to lock a backup (for e.g. safely poking around in the
.index on a live system).

example failure:
$bu_lock -f /path/to/backup
* Trying to obtain lock on /path/to/backup...
NO some error
<EOF>

example success:
$bu_lock -f /path/to/backup
* Trying to obtain lock on /path/to/backup...
[potentially a delay here if we need to wait for another process to release the lock]
OK locked
[bu_lock waits for its stdin to close, then unlocks and exits]

if you need to rummage around in backup.index, run this program in another
shell, do your work, then ^D it when you're finished.

you could also call this from e.g. perl over a bidirectional pipe - wait to
read "OK locked", then you've got your lock.  close the pipe to unlock when
you're finished working.  if you don't read "OK locked" before the pipe closes
then something went wrong and you didn't get the lock.

specify backups by -f filename, -m mailbox, -u userid
default run mode as above
-s to fork an sqlite of the index (and unlock when it exits)
-x to fork a command of your choosing (and unlock when it exits)


reconstruct
-----------

* compress/reindex are probably both invocations of a backup reconstruct tool
  rather than distinct tools
* functionality for re-ordering a backup whose data is in funky order?
  it shouldn't be able to get out of order if it's only being written to by
  backupd but if you've got funky home-grown stuff around the edges, or you're
  working on the backup system itself, ...


ctl_backupsdb
-------------

sbin tool for dealing with backups, their indexes, and the main database

needs:
    * rebuild backups.db from disk contents
    * list backups
    * rename a backup
    * delete a backup
    * verify a backup (check all sha1's, not just most recent)

not sure if these should be included, or separate tools:
    * reindex a backup (or more)
    * compress a backup (or more)
    * lock a backup

usage:
    ctl_backupsdb [options] -R                              # reconstruct backups.db from disk files
    ctl_backupsdb [options] -l [[mode] backup...]           # list backup locations for given/all users
    ctl_backupsdb [options] -r new_fname [mode] backup      # rename a backup (think about this more)
    ctl_backupsdb [options] -d [mode] backup                # delete a backup
    ctl_backupsdb [options] -V [mode] backup...             # verify specified backups
    ctl_backupsdb [options] -i [mode] backup...             # reindex specified backups
    ctl_backupsdb [options] -z [mode] backup...             # compress specified backups
    ctl_backupsdb [options] -L [lock_opts] [mode] backup    # lock specified backup

options:
    -C alt_config       # alternate config file
    -v                  # verbose

mode:
    -A                  # all known backups (not valid for single backup commands)
    -f                  # specified backups interpreted as filenames
    -m                  # specified backups interpreted as mboxnames
    -u                  # specified backups interpteted as userids (default)

lock_opts:
    -s                  # lock backup and open index in sqlite
    -x cmd              # lock backup and execute cmd
    -p                  # lock backup and wait for eof on stdin (default)


partitions
----------

not enough information in sync protocol to handle partitions easily?

we know what the partition is when we do an APPLY operation (mailbox, message,
etc), but the initial GET operations don't include it.  so we need to already
know where the appropriate backup is partitioned in order to find the backup
file in order to look inside it to respond to the GET request

if we have a mailboxes database (indexed by mboxname, uniqueid and userid) then
maybe that would make it feasible?  if it's not in the mailboxes database then
we don't have a backup for it yet, so we respond accordingly, and get sent
enough information to create it.

does that mean the backup api needs to take an mbname on open, and it handles
the job of looking it up in the mailboxes database to find the appropriate
thing to open?

can we use sqlite for such a database, or is the load on it going to be too
heavy?  locking?  we have lots of database formats up our sleeves here, so
even though we use sqlite for the backup index there isn't any particular
reason we're beholden to it for the mailboxes db too

if we have a mailboxes db then we need a reconstruct tool for that, too


installation instructions
-------------------------

(obviously, most of this won't work at this point, because the code doesn't
exist.  but this is, approximately, where things are heading.)

on your backup server:
    * compile with --enable-backup configure option and install
    * imapd.conf:
        backup_data_path: /var/spool/backup
        backups_db: twoskip
        backups_db_path: /var/imap/backups.db
    * cyrus.conf SERVICES:
        backupd cmd="backupd" listen="csync" prefork=0
        (remove other services, most likely)
        (should i create a master/conf/backup.conf example file?)
    * cyrus.conf EVENTS:
        bu_compress cmd="bu_compress -A" at=0400
    * start server as usual
    * do i want a special port for backupd?

on your imap server:
    * imapd.conf:
        sync_log_channels: backup
        sync_log: 1
        backup_sync_host: backup-server.example.com
        backup_sync_port: csync
        backup_sync_authname: ...
        backup_sync_password: ...
        backup_sync_repeat_interval: ... # seconds, smaller value = livelier backups but more i/o
        backup_sync_shutdown_file: ....
    * cyrus.conf STARTUP:
        backup_sync cmd="sync_client -r -n backup"
    * cyrus.conf SERVICES:
        restored cmd="restored" [...]
    * start/restart master

files and such:
    {configdirectory}/backups.db                    - database mapping userids to backup locations
    {backup_data_path}/<hash>/<userid>_XXXXXX       - backup data stream for userid
    {backup_data_path}/<hash>/<userid>_XXXXXX.index - index into userid's backup data stream

do i want rhost in the path?
    * protects from issue if multiple servers are trying to back up their own version of same user
      (though this is its own problem that the backup system shouldn't have to compensate for)
    * but makes location of undifferentiated user unpredictable
    * so probably not, actually


chatting about implementation 20/10
-----------------------------------
09:54 elliefm_
here's a fun sync question
APPLY MESSAGE provides a list of messages
can a single APPLY MESSAGE contain messages for multiple mailboxes and/or users?
my first hunch is that it doesn't cross users, since the broadest granularity for a single sync run is USER
10:06 kmurchison
We'd have to check with Bron, but I *think* messages can cross mailboxes for a single user
10:06 brong_
yes
APPLY MESSAGE just adds it to the reserve list
10:07 elliefm_
nah apply message uploads the message, APPLY RESERVE adds it to the reserve list :P
10:07 brong_
same same
APPLY RESERVE copies it from a local mailbox
APPLY MESSAGE uploads it
10:07 elliefm_
yep
10:07 brong_
they both wind up in the reserve list
10:07 elliefm_
ahh i see what you mean, gotcha
10:07 brong_
until you send a RESTART
ideally you want it reserve in the same partition, but it will copy the message over if it's not on the same partition
there's no restriction on which mailbox it came from/went to
good for user renames, and good for an append to a bunch of mailboxes in different users / shared space all at once
(which LMTP can do)
10:10 elliefm_
i can handle the case where a single APPLY MESSAGE contains messages for multiple mailboxes belonging to the same user
but i'm in trouble if a single APPLY MESSAGE can contain messages belonging to different users
10:14 brong_
elliefm_: why?
10:14 brong_
you don't have to keep them if they aren't used
10:15 elliefm_
for backups - when i see the apply, i need to know which user's backup to add it to.  that's easy enough if it doesn't cross users but gets mega fiddly if it does
i'm poking around in sync client to see if it's likely to be an issue or not
11:00 brong__
elliefm_: I would stage it, and add it to users as it gets refcounted in by an index file
11:07 elliefm_
that's pretty much what we do for ordinary sync and delivery stuff yeah?
11:08 brong__
yep
and it's what the backup thing does
11:09 elliefm_
i'm pretty sure that APPLY RESERVE and APPLY MESSAGE don't give a damn about users, they're just "here's every message you might not have already had since last time we spoke" and it lets the APPLY MAILBOX work out where to attach them later
11:09 brong__
yep
11:09 elliefm_
so yeah, i'll need to do something here
i've been working so far on the idea that a single user's backup consists of 1) an append-only gzip stream of the sync protocol chat that built it, and 2) an index that tracks current state of mailboxes, and offsets within (1) of message data
that gets us good compression (file per user, not file per message), and if the index gets corrupted or lost, it's rebuildable purely from (1), it doesn't need a live copy of the original mailbox
11:12 brong_
yep, that all works
11:12 elliefm_
(so if you lose your imap server, you're not unable to rebuild a broken index on the backup)
11:13 brong_
it's easy enough to require the sync protocol stream to only contain messages per user
though "apply reserve" is messy
because you need to return "yes, I have that message"
11:13 elliefm_
with that implementation i can't (easily) keep user.a's messages from not existing in user.b's data stream (though they won't be indexed)
11:14 brong_
I'm not too adverse to the idea of just unpacking each message as it comes off the wire into a temporary directory
11:14 elliefm_
(because at the time i'm receiving the sync data i don't know which it needs to go in, so if they come in in the same reserve i'd need to append them to both data streams)
which isn't a huge problem, just… irks me a bit
11:14 brong_
and then reading the indexes as they come in, checking against the state DB to see if we already have them, and streaming them into the gzip if they aren't there yet
what we can do is something like the current format, where files go into a tar
11:16 elliefm_
i guess the fiddly bit there is that there's one more moving part to keep synchronised across failure states
a backup for a single user becomes 1) data stream + 2) any messages that were uploaded but not yet added to a mailbox + 3) index (which doesn't know what to do with (2))
which in the general case is fine, the next sync will update the mailboxes, which will push (2) into (1) and index it nicely, and on we go
but it's just a little bit more mess if there's a failure that you need to recover from between those states — it's no longer a simple case of "it's in the backup and we know everything about it" or "it doesn't exist", there's a third case of "well we might have the data but don't really know what to do with it"
the other fiddly bit is that the process of appending to the data stream is suddenly in the business of crafting output rather than simply dumping what it gets, which isn't really burdensome, but it is one more little crack for bugs to crawl into
i guess in terms of sync protocol, one thing i could do on my end is identify apply operations that seem to contain multiple users' data, and just return an error on those.  the sync client on the other end will promote them until they're eventually user syncs, which i think are always user granularity
11:50 elliefm_
i think for now, first stage implementation will be to stream the reserve/message commands in full to every user backup they might apply to.  and optimising that down so that each stream only contains messages belonging to that user can be a future optimisation
